{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in building a language model over a language with three words: A, B, C. Our training corpus is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = \"AAABACBABBBCCACBCC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "**First train a unigram language model using maximum likelihood estimation. What are the probabilities? (Just leave in the form of a fraction)? Reminder: We don't need start or end tokens for training a unigram model, since the context of each word doesn't matter. So, we will not add any special tokens to our corpus for this part of the problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'A': 6, 'C': 6, 'B': 6})\n",
      "Total corpus length is 18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "unigram_counts = Counter(train_corpus)\n",
    "\n",
    "print(unigram_counts)\n",
    "print(\"Total corpus length is %d\" % len(train_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(A) = $\\frac{6}{18} = \\frac{1}{3}$**\n",
    "\n",
    "**P(B) = $\\frac{1}{3}$**\n",
    "\n",
    "**P(C) = $\\frac{1}{3}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "**Next train a bigram language model using maximum likelihood estimation. For this problem, we will add an end token, $, at the end of the string, so that we can model the probability of the sentence ending after a particular word. If you chose to add a start token as well, that's fine too, but these solutions assume no start token. Fill in the probabilities below. Leave your answers in the form of a fraction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAABACBABBBCCACBCC$\n",
      "{('B', 'C'): 2, ('A', 'A'): 2, ('B', 'B'): 2, ('B', 'A'): 2, ('C', 'A'): 1, ('A', '$'): 0, ('C', 'C'): 2, ('C', 'B'): 2, ('A', 'B'): 2, ('A', 'C'): 2, ('C', '$'): 1, ('B', '$'): 0}\n"
     ]
    }
   ],
   "source": [
    "# add end token to corpus\n",
    "extended_train_corpus = train_corpus + '$'\n",
    "print(extended_train_corpus)\n",
    "\n",
    "# set up bigram table with zeros for every possible bigram from our corpus\n",
    "bigram_counts = {(token1, token2):0 for token1 in train_corpus for token2 in extended_train_corpus}\n",
    "\n",
    "# count bigrams that actually occur within our corpus\n",
    "corpus_bigram_counts = Counter(zip(train_corpus, extended_train_corpus[1:]))\n",
    "\n",
    "# add these bigram counts to our bigram table\n",
    "for bigram in corpus_bigram_counts.keys():\n",
    "    bigram_counts[bigram] += corpus_bigram_counts[bigram]\n",
    "\n",
    "print(bigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each bigram probability should be the bigram count divided by the unigram count of the first token. In our corpus, every unigram count is 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(A|A) = $\\frac{2}{6} = \\frac{1}{3}$**\n",
    "\n",
    "**P(A|B) = $\\frac{2}{6} = \\frac{1}{3}$**\n",
    "\n",
    "**P(A|C) = $\\frac{1}{6}$**\n",
    "\n",
    "**P(B|A) = $\\frac{2}{6} = \\frac{1}{3}$**\n",
    "\n",
    "**P(B|B) = $\\frac{2}{6} = \\frac{1}{3}$**\n",
    "\n",
    "**P(B|C) = $\\frac{2}{6} = \\frac{1}{3}$**\n",
    "\n",
    "**P(C|A) = $\\frac{2}{6} = \\frac{1}{3}$**\n",
    "\n",
    "**P(C|B) = $\\frac{2}{6} = \\frac{1}{3}$**\n",
    "\n",
    "**P(C|C) = $\\frac{2}{6} = \\frac{1}{3}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "**Now evaluate your language models on the corpus \"ABACABB\". What is the perplexity of the unigram language model evaluated on this corpus? Since we didn't add any special start/end tokens when we were training our unigram language model, we won't add any when we evaluate the perplexity of the unigram language model, either, so that we're consistent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Turn unigram counts table into unigram probability table by dividing\n",
    "each unigram count by the total number of words in the training corpus\n",
    "\"\"\"\n",
    "unigram_probs = {unigram: unigram_counts[unigram] / float(len(train_corpus)) \\\n",
    "                 for unigram in unigram_counts.keys()}\n",
    "\n",
    "test_sent = \"ABACABB\"\n",
    "N = len(test_sent)\n",
    "\n",
    "\"\"\"\n",
    "Approximate the joint probability P(w1w2...wn) on the test sentence\n",
    "using the product of unigram probabilities P(w1)*P(w2)*...*P(wn)\n",
    "\"\"\"\n",
    "joint_prob_unigram_estimate = 1\n",
    "for unigram in test_sent:\n",
    "    joint_prob_unigram_estimate *= unigram_probs[unigram]\n",
    "    \n",
    "# perplexity = 1 / P(w1w2...wn)^(1/n)\n",
    "unigram_perplexity = 1.0 / (joint_prob_unigram_estimate)**(1.0/N)\n",
    "print(unigram_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the perplexity of the bigram language model evaluated on this corpus? Since we added an end token when we were training our bigram model, we'll add an end token to this corpus again before we evaluate perplexity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint probability bigram estimate: 0.000000\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2df8e9486276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# perplexity = 1 / P(w1w2...wn)^(1/n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mbigram_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjoint_prob_bigram_estimate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bigram language model perplexity: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbigram_perplexity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "\"\"\"Turn bigram counts table into bigram probabilities by dividing by unigram counts\"\"\"\n",
    "bigram_probs = dict(bigram_counts)  # copy\n",
    "for bigram in corpus_bigram_counts.keys():  # don't have unigram counts for end token\n",
    "    first_token = bigram[0]\n",
    "    bigram_probs[bigram] /= float(unigram_counts[first_token])\n",
    "\n",
    "extended_test_sent = test_sent + '$'\n",
    "N = len(extended_test_sent)\n",
    "    \n",
    "\"\"\"\n",
    "Approximate the joint probability P(w1w2...wn) on the test sentence\n",
    "using the product of bigram probabilities P(w)*P(w2)*...*P(wn)\n",
    "\"\"\"\n",
    "joint_prob_bigram_estimate = 1\n",
    "for bigram in zip(test_sent, extended_test_sent[1:]):\n",
    "    joint_prob_bigram_estimate *= bigram_probs[bigram]\n",
    "    \n",
    "print(\"Joint probability bigram estimate: %f\" % joint_prob_bigram_estimate)\n",
    "    \n",
    "# perplexity = 1 / P(w1w2...wn)^(1/n)\n",
    "bigram_perplexity = 1.0 / (joint_prob_bigram_estimate)**(1.0/N)\n",
    "print(\"Bigram language model perplexity: %f\" % bigram_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this Zero Division Error actually makes sense. The joint probability estimate should be 0, because the last bigram of our test sentence is 'B$'. This bigram actually never occurred in our training corpus, and so it has a count and probability of 0. Thus the perplexity is undefined with this naive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "**Now repeat everything above for add-1 (Laplace) smoothing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, Laplace smoothing should fix our zero division error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace unigram perplexity: 3.000000\n"
     ]
    }
   ],
   "source": [
    "# Define relevant N and V values from the training corpus\n",
    "n_train_corpus = len(train_corpus)  # N, number of tokens in training corpus\n",
    "v_train_corpus = len(set(train_corpus))  # V, number of unique tokens in training corpus\n",
    "\n",
    "# Add 1 to every unigram count\n",
    "laplace_unigram_counts = {unigram:c+1 for unigram,c in unigram_counts.iteritems()}\n",
    "\n",
    "\"\"\"\n",
    "Create unigram probabilities as before, using (c + 1) / (N + V)\n",
    "\"\"\"\n",
    "laplace_unigram_probs = {unigram: c / float(n_train_corpus + v_train_corpus) \\\n",
    "                         for unigram,c in laplace_unigram_counts.iteritems()}\n",
    "\n",
    "test_sent = \"ABACABB\"\n",
    "N = len(test_sent)\n",
    "\n",
    "\"\"\"\n",
    "Approximate the joint probability P(w1w2...wn) on the test sentence\n",
    "using the product of unigram probabilities P(w1)*P(w2)*...*P(wn)\n",
    "\"\"\"\n",
    "joint_prob_laplace_unigram_estimate = 1\n",
    "for unigram in test_sent:\n",
    "    joint_prob_laplace_unigram_estimate *= laplace_unigram_probs[unigram]\n",
    "    \n",
    "# perplexity = 1 / P(w1w2...wn)^(1/n)\n",
    "laplace_unigram_perplexity = 1.0 / (joint_prob_laplace_unigram_estimate)**(1.0/N)  # different N (length of test sentence)\n",
    "print(\"Laplace unigram perplexity: %f\" % laplace_unigram_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint probability laplace bigram estimate: 0.010417\n",
      "Laplace bigram language model perplexity: 1.769228\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create bigram probabilities as before, using (c_bi + 1) / (c_uni + V)\n",
    "\"\"\"\n",
    "laplace_bigram_probs = {bigram:c+1 for bigram,c in bigram_counts.iteritems()}  # copy counts, and add-1\n",
    "\n",
    "for bigram in corpus_bigram_counts.keys():  # don't have unigram counts for end token\n",
    "    first_token = bigram[0]\n",
    "    laplace_bigram_probs[bigram] /= float(unigram_counts[first_token])\n",
    "\n",
    "extended_test_sent = test_sent + '$'\n",
    "N = len(extended_test_sent)\n",
    "    \n",
    "\"\"\"\n",
    "Approximate the joint probability P(w1w2...wn) on the test sentence\n",
    "using the product of bigram probabilities P(w)*P(w2)*...*P(wn)\n",
    "\"\"\"\n",
    "joint_prob_laplace_bigram_estimate = 1\n",
    "for bigram in zip(test_sent, extended_test_sent[1:]):\n",
    "    joint_prob_laplace_bigram_estimate *= laplace_bigram_probs[bigram]\n",
    "    \n",
    "print(\"Joint probability laplace bigram estimate: %f\" % joint_prob_laplace_bigram_estimate)\n",
    "    \n",
    "# perplexity = 1 / P(w1w2...wn)^(1/n)\n",
    "laplace_bigram_perplexity = 1.0 / (joint_prob_laplace_bigram_estimate)**(1.0/N)\n",
    "print(\"Laplace bigram language model perplexity: %f\" % laplace_bigram_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that laplace-smoothed bigram language model is less perplexed than the unigram model, indicating it is the better choice for this test sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Group Work with Person X, Person Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "**What is the difference between using an UNK token (for unknown words) and smoothing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give an example where you would use one versus the other.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "**Suppose you build an interpolated trigram language model, with three weights $\\lambda_1$ for unigrams, $\\lambda_2$ for bigrams, and $\\lambda_3$ for trigrams. Normally we set these lambdas on a held-out set. Suppose instead we set them on the training data. This will cause the lambdas to take on very unusual values. What will these lambdas look like? Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "**Show that if we estimate two bigram language models using unsmoothed relative frequencies (MLE), one from a text corpus and the second from the same corpus in reverse order, the models will assign the same probability to new sentences (when applied in forward and backward order respectively). Hint: First write out the entire equation for sentence probabilities in terms of counts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
